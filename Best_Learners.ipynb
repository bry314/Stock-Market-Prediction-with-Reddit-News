{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries afddhda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('finished_without_irregular.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =dataset[dataset['Date'] < '2015-01-02']\n",
    "test_dataset = dataset[dataset['Date'] >= '2015-01-02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english',\n",
    "                             min_df=0.02, \n",
    "                             max_df=0.70, \n",
    "                             max_features=20000, \n",
    "                             ngram_range=(1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = count_vectorizer.fit_transform(X_train_dataset)\n",
    "X_test_vec = count_vectorizer.transform(X_test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt_train = dt.fit(X_train_vec,y_train_dataset)\n",
    "\n",
    "optimal = SelectFromModel(dt_train, prefit=True)\n",
    "X_reduced_train_vec=optimal.transform(X_train_vec)\n",
    "X_reduced_test_vec=optimal.transform(X_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score for mnb with training data is 0.56678321283865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = MultinomialNB()\n",
    "mnb_param = {\n",
    "  'alpha': np.linspace(1, 5, 1),\n",
    "  'fit_prior': [True, False],  \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "best_mnb = GridSearchCV(estimator = model, \n",
    "                    cv = 10,\n",
    "                    scoring='roc_auc',\n",
    "                    param_grid = mnb_param)\n",
    "\n",
    "\n",
    "# train the model using all training set\n",
    "best_mnb.fit(X_reduced_train_vec, y_train_dataset)\n",
    "print(\"best score for mnb with training data is \"+str(best_mnb.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score for tree with training data is 0.5687505655522713\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(random_state = 2)\n",
    "\n",
    "# list(range(15, 40, 2)\n",
    "tree_param = {'max_leaf_nodes': [15, 20, 25, 30, 35, 40],\n",
    "             'max_depth': [5, 10, 15, 20, 25, 30],\n",
    "             'criterion' : ['gini', 'entropy']}\n",
    "\n",
    "\n",
    "best_tree = GridSearchCV(estimator = tree, \n",
    "                    cv = 10,\n",
    "                    scoring='roc_auc',\n",
    "                    param_grid = tree_param)\n",
    "\n",
    "\n",
    "# train the model using all training set\n",
    "best_tree.fit(X_reduced_train_vec, y_train_dataset)\n",
    "print(\"best score for tree with training data is \"+str(best_tree.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score for lr with training data is 0.5769079297863945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "lr_param = [\n",
    "  {'penalty': ['l1'], 'C': np.logspace(0, 4, 10), 'solver': ['liblinear', 'saga']},\n",
    "  {'penalty': ['l2'], 'C': np.logspace(0, 4, 10), 'solver': ['newton-cg', 'lbfgs']},\n",
    " ]\n",
    "\n",
    "lr_model = LogisticRegression(random_state=0, max_iter=1000)\n",
    "\n",
    "best_lr = GridSearchCV(estimator = lr_model, \n",
    "                    cv = 10,\n",
    "                    scoring='roc_auc',\n",
    "                    param_grid = lr_param)\n",
    "\n",
    "\n",
    "# train the model using all training set\n",
    "best_lr.fit(X_reduced_train_vec, y_train_dataset)\n",
    "print(\"best score for lr with training data is \"+str(best_lr.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#create a new random forest classifier\n",
    "rf = RandomForestClassifier(bootstrap=True, class_weight=\"balanced_subsample\")\n",
    "\n",
    "#create a dictionary of all values we want to test for n_estimators, number of trees\n",
    "params_rf = {'n_estimators': [100, 200, 300, 400, 500]}\n",
    "\n",
    "#use gridsearch to test all values for n_estimators\n",
    "best_rf = GridSearchCV(rf, \n",
    "                     params_rf, \n",
    "                     cv=10, \n",
    "                     scoring='roc_auc')\n",
    "\n",
    "#fit model to training data\n",
    "best_rf.fit(X_reduced_train_vec, y_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "abc = AdaBoostClassifier()\n",
    "\n",
    "# Ada Boost Classifier\n",
    "params_abc = {'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "            'n_estimators': [50, 100, 200, 300, 400, 500, 600]}\n",
    "\n",
    "#use gridsearch to test all values for n_estimators\n",
    "best_abc = GridSearchCV(abc,\n",
    "                     params_abc, \n",
    "                     cv=10, \n",
    "                     scoring='roc_auc')\n",
    "\n",
    "best_abc.fit(X_reduced_train_vec, y_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('mnb', GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'alpha': array([1.]), 'fit_prior': [True, False]},\n",
       "       pre_dispatch='2*n_jobs',...e_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0))],\n",
       "         flatten_transform=None, n_jobs=None, voting='hard', weights=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "estimators=[('mnb', best_mnb), ('lr', best_lr), ('dt', best_tree)]\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators, voting='hard')\n",
    "ensemble.fit(X_reduced_train_vec, y_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def roc(model):\n",
    "    y_predict_dataset = model.predict(X_reduced_test_vec)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_dataset, y_predict_dataset, pos_label=1)\n",
    "    return auc(fpr, tpr)\n",
    "def accuracy(model):\n",
    "    y_predict_dataset = model.predict(X_reduced_test_vec)\n",
    "    accuracy = accuracy_score(y_test_dataset, y_predict_dataset, normalize=True, sample_weight=None)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB auc:  0.5068\n",
      "Decision Tree auc:  0.57342\n",
      "Logistic Regression auc 0.51739\n",
      "Random Forest auc:  0.51714\n",
      "Ada Boost Classifier auc:  0.50378\n",
      "Ensemble auc:  0.52503\n"
     ]
    }
   ],
   "source": [
    "print(\"MNB auc: \", round(roc(best_mnb), 5))\n",
    "print(\"Decision Tree auc: \", round(roc(best_tree), 5))\n",
    "print(\"Logistic Regression auc\", round(roc(best_lr), 5))\n",
    "print(\"Random Forest auc: \", round(roc(best_rf), 5))\n",
    "print(\"Ada Boost Classifier auc: \", round(roc(best_abc), 5))\n",
    "print(\"Ensemble auc: \", round(roc(ensemble), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB accuracy:  0.50794\n",
      "Decision Tree accuracy:  0.57407\n",
      "Logistic Regression accuracy 0.51852\n",
      "Random Forest accuracy 0.52116\n",
      "Ada Boost Classifier accuracy 0.50529\n",
      "Ensemble accuracy 0.52646\n"
     ]
    }
   ],
   "source": [
    "print(\"MNB accuracy: \", round(accuracy(best_mnb), 5))\n",
    "print(\"Decision Tree accuracy: \", round(accuracy(best_tree), 5))\n",
    "print(\"Logistic Regression accuracy\", round(accuracy(best_lr), 5))\n",
    "print(\"Random Forest accuracy\", round(accuracy(best_rf), 5))\n",
    "print(\"Ada Boost Classifier accuracy\", round(accuracy(best_abc), 5))\n",
    "print(\"Ensemble accuracy\", round(accuracy(ensemble), 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
